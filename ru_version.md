# Data Science.  Взгляд на начинающих ML/DS разработчиков с точки зрения компании.

### [2021] Аудитория – студенты. Опыт – 2-4 курс IT факультета.
---
[Знакомство. Ответы на заранее подготовленные вопросы студентов]

---


## - С чего начать свой путь в ML/DS? Чему приходится доучивать джунов?


Начать стоит с изучения утилит и сервисов, используемых в продуктовой разработке! Вам должны быть как минимум знакомы такие слова как git, Jira, docker, AWS. Необходимо хорошо знать язык программирования, который вы будете использовать – в основном это Python, R, или Scala, а так же SQL. Библиотеки numpy, scipy, pandas в случае Python. Основы статистики и линаглебры.

Я не стал бы сильно заострять внимание на математической части и даже не библиотеках. Скажем честно – для многих присутствующих, весь текущий опыт в DS – это просмотр нескольких курсов по теории, с последующим запуском одиночного Python скрипта на [Kaggle](https://www.kaggle.com/), где на вход принимается относительно чистый CSV файл, и результат скрипта – это ещё один CSV файл.

Проблема в том, что эта среда слишком стерильна. В реальности более половины времени уходит на препроцессинг входных данных, и я говорю не про анализ фич, а в принципе их создание из абстрактного массива разнородных данных. Иногда их необходимо предварительно спарсить, и для этого надо знать Scrapy и lxml. К ним подтягивается условная БД для хранения данных и `docker-compose` для объединения всех частей парсера и его запуска. 

Очень сложно представить на крупном проекте изолированного DS человека, который выпрашивает готовые объединённые таблицы из продуктовой базы данных у девопса или ещё кого, потом пишет одиночный .py скрипт со строками:

```python
data = pd.read_csv(“data.csv”)
…
model = Classifier()
model.fit(X, y)
predictions = model.predict(X_test)
```

и кидает его в канал бэкенда в слаке со словами “моя работа закончена, дальше сами интегрируйте”.

Подобная изоляция возможно только если этот аналитик является квантом с PhD в финтехе, к которому приставлена команда, которая переводит его черновики с R или TeX варианта на ЯП используемые в проете, эта же команда адаптирует наброски под подходящие фреймворки, занимается тестированием...

Подводя итог – в целом доучивать приходится всему, что происходит на бэкенде, пусть и в более упрощённой форме – умение заворачивать скрипты в микросервисы, принципы REST, стандарты оформления кода, процесс MR, тестирование, развёртывание проекта, самостоятельное получение необходимых данных… Что касается части ML, то здесь, как правило, особых знаний и не требуется и достаточно базового понимания того, что используешь. Опять же, если это не финтех.

---

## - Чем отличается DS подход в финтехе?

Основное отличие в формате данных и в целях обучения модели. В финтехе как правило все данные уже структурированы и аналитик получает на входе то, что вы могли видеть на Kaggle. Просто потому что достаточно глупо тратить время человека с $200k+ контрактом на очистку и сбор данных. Здесь же начинается и типичный "Kaggle подход" улучшения  – поиск наиболее оптимальных параметров полным перебором, выбор моделей, анализ распределений, отбор признаков, регуляризация, ансамбли моделей итд. Потому что цель – это улучшить показатель какого-нибудь коэффициента Шарпа с условных 0.21533 до 0.21600. Основная логика получения данных и даже создания признаков скрыта из соображений безопасности, и, конкретно в этом случае, изолированная среда кванта полностью оправдана.

---

## - Если не брать в расчёт финтех, то как улучшать модель? Есть примеры ML стека в проде?

На самом деле всё абсолютно так же - препроцессинг, учёт выбросов, распределений, алгоритмов, фич, построение PoC модели... в принципе, никто вам не запрещает после запуска PoC из 20 строк начать пару спринтов крутить веса и прочие опции, строить ансамбли из моделей... И, да, это можеть дать прирост в какую-то долю процента метрики. Вопрос лишь в выхлопе от этих манипуляций, так называемое КПД. Предположим, что у вас есть интернет-магазин, к которому необходимо добавить рекомендацию. Если следовать методичкам, то в базовом варианте – это получение из бд или через API данных, достаточных для создания матрицы “покупатель <> товар + оценка товара на пересечении”. После чего часто начинается цикл из перебора метрик расстояния, будь то косинусовая, евклидова или пирсона мера, снижение размерности (когда внезапно становится ясно, что такая матрица не влезет во всю оперативную память мира при релизе на полном наборе данных). Далее идёт перебор алгоритмов снижения размерности, полный перебора всевозможных опций этих алгоритмов, так называемая гипероптимизация, и всё в этом духе. Вся эта радость тестируется в рамках своего микросервиса на исторических данных и презентуется на каждом спринте в виде слайда: “Обновлены веса модели, метрика Something@k +0.016% от прошлого результата”.

Единственная проблема только в том, что потрачен уже н-ный месяц разработки, а глобального ничего не улучшается – продажи падают, пользователи рекомендованные товары не выбирают, и в целом зрительно они выглядят нерелевантными. Осознание этого факта обычно знаменует выход дата саентиста за пределы улучшения модели. Начинается проверка по чек-листу:

*	Проверка того, что модель действительно пошла в прод в том виде, в котором её задумал DS (Требуется знание ЯП проекта, умение ковырять API прода через `Postman`, а так же умение взаимодействовать с лидом бэкенда и девопсом). Как можно избежать этого?  - DS сидит на мерж реквесте интеграции своего сервиса в проект и активно участвует в нём на месте проверяющего, либо сам создаёт MR.

* 	Тестирование на исторических данных может быть некорректно в силу того, что в давние времена покупателям с бэка приходило 10 товаров по старой системе рекомендации, но фронт взял на себя ответственность не показывать последние 5, так как вёрстку ломало, ну и нерелевантны же зрительно. (Требуется знание истории проекта и взаимодействие DS со всеми отделами) Подобное решается ведением снапшотов проекта, и требует участия всей команды разработки.

*	Некорректна метрика / для создание метрики не хватает данных. (Опять же требуется понимание проекта целиком, а так же знание пути, который проходит покупатель) К примеру, можно создать тикет с реквестом на создание кнопки “нерелевантно” рядом с товаром или добавить аналитику кликов, что даст дополнительные позитивные и негативные сигналы для модели. Так же имеет смысл спросить у отдела продаж или менеджеров, что с их точки зрения является метрикой олицетворяющей “успех” - это может быть скорость роста продаж, количество новых клиентов, или какие-то совершенно неожиданные соотношения чего-либо к чему-то. Не факт, что эту метрику нужно будет использовать для оптимизации параметров, но весьма полезно смотреть на корреляцию метрики “успеха” с выбранной метрикой обучения.

*	 Необходимы дополнительные микросервисы – к примеру, можно вспомогательно смотреть на кластеры товаров, для чего может потребоваться NLP, после чего советовать товары из одного кластера и иметь возможность отфильтровывать нерелевантные перед дальнейшей обработкой. Или ещё лучше взять готовую универсальную классификацию наподобие UNSPSC и выбрав любой классификатор (уже не повторяя подход с бесконечной гипероптимизацией) создать вспомогательные датасеты, начать аугментировать данные. В целом усложнить процесс рекомендаций – продукт может рекомендоваться из схемы “Покупатель часто брал продукты производителя Х → рекомендуем другие продукты с высокими оценками производителя Х из категории Y в которой покупатель был активен” Дополнительные данные делим в пропорции 70% - семантически схожи с продуктами, которые были куплены или которые были кликнуты мышью или как-то иначе позитивно отмечены на UI покупателем + 30% случайных популярных на данный момент товары из категории Y. Добавляем учёт цен, скидок, сезонности, прочих групп, будь то совпадение страны производителя и покупателя, наличие доставки, время дня, наличие или отсутствие отзывов, и так далее, и так далее. (И вот для этого требуется знание сферы, в которой работает DS).

Касательно вопроса о стеке – всё ранее сказанное, опуская библиотеки, может быть представлено в виде
    
`DataLake` → `MLFlow`/`AWS_Sagemaker`(`docker_image`(`fastAPI`(`model`)→`REST`)) <-> core app. + `Elasticsearch` + BI tool (`looker`/`tableau`) + `Datadog` + `Grafana`

\+ подключение `Apache software` в случае высокой нагрузки, но я не видел случаев, где условные `spark` или `hadoop` были бы реально нужны, очень часто мощность потока данных переоценивается при выборе инструментария обработки. По инфраструктуре - CI/CD, версионирование и релиз моделей в целом, на человеке, которого мы будем звать MLOps, роуты A/B тестирования и снапшоты скорее на DevOps и бэкенд-команде. По итогу всё это позволяет выстроить 3 уровня аналитики – уровень платформы, уровень команды разработки и продаж, и уровень наблюдения за компанией (информация для топ менеджеров и инвесторов).

К чему всё это было сказано - выполнение любого пункта из этого чек-листа зачастую даёт гораздо больше пользы, чем улучшение отдельно взятой модели на 0.000х% accuracy. Поэтому при наборе в DS меня обычно больше интересовал ход мысли кандидата и умения, которые казалось бы не относятся к DS сфере, но по факту относятся. В каком-то роде проще научить бэкендера мудростям аналитики, чем наоборот.

Не могу сказать, что полностью игнорирую математические скилы, например считаю очень полезным понимание распределений, выбросов, геометрической интерпретации классификации и регрессии, а так же умение выбора подходящих метрик и алгоритмов. Но в вопроса рода “Как строятся деревья в градиентном бустинге?” большого смысла не вижу.

---

## - Что добавляли в тестовое задание?

Что-то практическое, к примеру очистка данных и их стандартизация. Многое можно сказать о ходе мыслей кандидата, смотря на методы очистки – можно их использовать повторно или нет, например. Были случаи, когда логика приведения данных в порядок выглядела как: “на седьмой строке удалить символы с 10 по 15”,  но были и универсальные функции с regexами, которые можно было хоть сейчас добавлять на рабочий проект. То есть в целом, это были довольно абстрактные задачи с полной свободой в их решении, где в максимальном варианте я бы хотел видеть докеризированое приложение, включающее в себя простенький парсер, модуль очистки и классификатор. Абсолютно всё могло быть скопировано с первой взятой ссылки с гугла по запросу "classificator + sklearn" и сделано менее чем за час.

Большое значение имеет качество кода и понимание поставленной цели. Бонусы за креативность, вроде аугментирования данных или подключение фреймворков, которые решают задачу в три строки. Проблема студентов, особенно олимпиадников, в том, что они пытаются зачем-то скрыть использование ответов со stackoverflow или копипаст с похожих готовых проектов с github'а, в ход идёт чуть ли не обфускация кода. Полностью выбрасывается мысль о том, что этот код также должны поддерживать и дополнять другие члены команды разработки. Заместо импорта одной библиотеки один раз видел имплементацию xgboost'a "на коленке" в 500+ строк кода. Причём сам скрипт не запускался, так как пути импорта модели требовали особое локальное окружение соискателя, а докер-образ встречается в 1 из 10 присылаемых решений, если не указывать это прямым текстом в задании. Примерно половина решений не имеет списка необходимых библиотек или запускается с ошибками, что требует доработок от проверяющих и это довольно времязатратно. В целом, что нужно понять - разница лабораторных и реальных задач в том, что в первом случае вам нужно самостоятельно с нуля разработать всё, что бы понять логику на самом низком уровне, а компании от вас требуется исключительно решение поставленной бизнесом задачи. Eсли она решается импортом известной библиотеки с добавлением пары строк кода – это только плюс. Больше времени на тестирование и внедрение.
